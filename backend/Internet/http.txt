First things first, what is HTTP? 

HTTP is a TCP/IP-based application layer communication protocol that standardizes how clients and servers communicate with each other.
It defines how content is requested and transmitted across the internet. By application layer protocol,
I mean that it’s simply an abstraction layer that standardizes how hosts (clients and servers) communicate. 
HTTP itself depends on TCP/IP to get requests and responses between the client and server. 
By default, TCP port 80 is used, but other ports can also be used. HTTPS, however, uses port 443.

HTTP was HTTP/0.9(1991).
    No headers
    GET was the only allowed method
    Response had to be HTML

HTTP/1.0 - 1996
    headers included
    GET/POST/HEAD methods
    response could be image, video, HTML, plain text or any other content type.
    each request has to made on a different connection like 20images, 2videos so 22 diff TCP connections.

    3-way handshake:
        Three-way handshake in it’s simples form is that all the TCP connections begin with a three-way
        handshake in which the client and the server share a series of packets before starting to share 
        the application data.

        SYN - Client picks up a random number, let’s say x, and sends it to the server.\

        SYN ACK - Server acknowledges the request by sending an ACK packet back to the 
        client which is made up of a random number, let’s say y picked up by server and 
        the number x+1 where x is the number that was sent by the client

        ACK - Client increments the number y received from the server and sends an ACK packet back with the number y+1

        Once the three-way handshake is completed, the data sharing between the client and server may begin. 
        It should be noted that the client may start sending the application data as soon as it dispatches 
        the last ACK packet but the server will still have to wait for the ACK packet to be recieved in order to fulfill the request.

        However, some implementations of HTTP/1.0 tried to overcome this issue by introducing a 
        new header called Connection: keep-alive which was meant to tell the server “Hey server, do not close this connection,
        I need it again”. But still, it wasn’t that widely supported and the problem still persisted.

        
        Apart from being connectionless, HTTP also is a stateless protocol i.e. server doesn’t maintain the information about the
        client and so each of the requests has to have the information necessary for the server to fulfill the request on it’s own 
        without any association with any old requests. And so this adds fuel to the fire i.e. apart from the large number of connections 
        that the client has to open, it also has to send some redundant data on the wire causing increased bandwidth usage.

HTTP/1.1 - 1997
    After merely 3 years of HTTP/1.0, the next version i.e. HTTP/1.1 was released in 1999; which made alot of improvements over it’s predecessor. 
    The major improvements over HTTP/1.0 included

    New HTTP methods were added, which introduced PUT, PATCH, OPTIONS, DELETE
    Hostname Identification In HTTP/1.0 Host header wasn’t required but HTTP/1.1 made it required.

    Persistent Connections As discussed above, in HTTP/1.0 there was only one request per connection and the connection was closed as soon as the request 
    was fulfilled which resulted in accute performance hit and latency problems. HTTP/1.1 introduced the persistent connections i.e. 
    connections weren’t closed by default and were kept open which allowed multiple sequential requests. 
    To close the connections, the header Connection: close had to be available on the request. Clients usually send this header in the 
    last request to safely close the connection.

    Pipelining It also introduced the support for pipelining, where the client could send multiple requests to the server without waiting for the response from server 
    on the same connection and server had to send the response in the same sequence in which requests were received. But how does the client know that this is the 
    point where first response download completes and the content for next response starts, you may ask! Well, to solve this, there must be Content-Length header present 
    which clients can use to identify where the response ends and it can start waiting for the next response.

    It should be noted that in order to benefit from persistent connections or pipelining, Content-Length header must be available on the response, because this would let the 
    client know when the transmission completes and it can send the next request (in normal sequential way of sending requests) or start waiting for the the next response (when pipelining is enabled).

    But there was still an issue with this approach. And that is, what if the data is dynamic and server cannot find the content length before hand? Well in that case, you really can’t benefit 
    from persistent connections, could you?! In order to solve this HTTP/1.1 introduced chunked encoding. In such cases server may omit content-Length in favor of chunked encoding (more to it in a moment). 
    However, if none of them are available, then the connection must be closed at the end of request.

    Chunked Transfers In case of dynamic content, when the server cannot really find out the Content-Length when the transmission starts, it may start sending the content in pieces (chunk by chunk) and 
    add the Content-Length for each chunk when it is sent. And when all of the chunks are sent i.e. whole transmission has completed, it sends an empty chunk i.e. the one with Content-Length set to zero 
    in order to identify the client that transmission has completed. In order to notify the client about the chunked transfer, server includes the header Transfer-Encoding: chunked.

    Unlike HTTP/1.0 which had Basic authentication only, HTTP/1.1 included digest and proxy authentication

    Caching

HTTP/2 - 2015
    By now, you must be convinced that why we needed another revision of the HTTP protocol. HTTP/2 was designed for low latency transport of content. 
    The key features or differences from the old version of HTTP/1.1 include

    Binary instead of Textual
    Multiplexing - Multiple asynchronous HTTP requests over a single connection
    Header compression using HPACK
    Server Push - Multiple responses for single request
    Request Prioritization
    Security

    Binary Protocol:

    HTTP/2 tends to address the issue of increased latency that existed in HTTP/1.x by making it a binary protocol. 
    Being a binary protocol, it easier to parse but unlike HTTP/1.x it is no longer readable by the human eye. 
    The major building blocks of HTTP/2 are Frames and Streams.

    Frames and Streams:

    HTTP messages are now composed of one or more frames. There is a HEADERS frame for the meta data and 
    DATA frame for the payload and there exist several other types of frames (HEADERS, DATA, RST_STREAM, SETTINGS, PRIORITY etc) 
    that you can check through the HTTP/2 specs.

    Every HTTP/2 request and response is given a unique stream ID and it is divided into frames. 
    Frames are nothing but binary pieces of data. A collection of frames is called a Stream. Each frame has a 
    stream id that identifies the stream to which it belongs and each frame has a common header. 
    Also, apart from stream ID being unique, it is worth mentioning that, any request initiated by client uses 
    odd numbers and the response from server has even numbers stream IDs.

    Apart from the HEADERS and DATA, another frame type that I think worth mentioning here is RST_STREAM which is a special 
    frame type that is used to abort some stream i.e. client may send this frame to let the server know that I don’t need this stream anymore. 
    In HTTP/1.1 the only way to make the server stop sending the response to client was closing the connection which resulted in increased latency 
    because a new connection had to be opened for any consecutive requests. While in HTTP/2, client can use RST_STREAM and stop receiving a specific 
    stream while the connection will still be open and the other streams will still be in play.

    Multiplexing:

    Since HTTP/2 is now a binary protocol and as I said above that it uses frames and streams for requests and responses, once a TCP connection is opened, 
    all the streams are sent asynchronously through the same connection without opening any additional connections. And in turn, the server responds 
    in the same asynchronous way i.e. the response has no order and the client uses the assigned stream id to identify the stream to which a specific packet belongs. 
    This also solves the head-of-line blocking issue that existed in HTTP/1.x i.e. the client will not have to wait for the request that is taking time and other requests 
    will still be getting processed.

    Header Compression:

    It was part of a separate RFC which was specifically aimed at optimizing the sent headers. The essence of it is that when we are constantly accessing the server 
    from a same client there is alot of redundant data that we are sending in the headers over and over, and sometimes there might be cookies increasing the headers 
    size which results in bandwidth usage and increased latency. To overcome this, HTTP/2 introduced header compression.

    Unlike request and response, headers are not compressed in gzip or compress etc formats but there is a different mechanism in place for header compression which is 
    literal values are encoded using Huffman code and a headers table is maintained by the client and server and both the client and server omit any repetitive headers 
    (e.g. user agent etc) in the subsequent requests and reference them using the headers table maintained by both.

    While we are talking headers, let me add here that the headers are still the same as in HTTP/1.1, except for the addition of some pseudo headers i.e. :method, :scheme, :host and :path.

    Server Push:

    Server push is another tremendous feature of HTTP/2 where the server, knowing that the client is going to ask for a certain resource, can push it to the client without even client asking for it. 
    For example, let’s say a browser loads a web page, it parses the whole page to find out the remote content that it has to load from the server and then sends consequent requests to the server to get that content.

    Server push allows the server to decrease the roundtrips by pushing the data that it knows that client is going to demand. How it is done is, server sends a special frame called PUSH_PROMISE notifying the client that, 
    “Hey, I am about to send this resource to you! Do not ask me for it.” The PUSH_PROMISE frame is associated with the stream that caused the push to happen and it contains the promised stream ID i.e. the stream on which 
    the server will send the resource to be pushed.

    Request Prioritization:

    A client can assign a priority to a stream by including the prioritization information in the HEADERS frame by which a stream is opened. At any other time, client can send a PRIORITY frame to change the priority of a stream.
    Without any priority information, server processes the requests asynchronously i.e. without any order. If there is priority assigned to a stream, then based on this prioritization information, server decides how much of the 
    resources need to be given to process which request.

    Security:

    There was extensive discussion on whether security (through TLS) should be made mandatory for HTTP/2 or not. In the end, it was decided not to make it mandatory. 
    However, most vendors stated that they will only support HTTP/2 when it is used over TLS. So, although HTTP/2 doesn’t require encryption by specs but it has kind of become mandatory by default anyway. With that out of the way, HTTP/2 when implemented over TLS does impose some requirementsi.e. TLS version 1.2 or higher must be used, there must be a certain level of minimum keysizes, ephemeral keys are required etc.



